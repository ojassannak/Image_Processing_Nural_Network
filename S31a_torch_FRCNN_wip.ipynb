{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "form",
    "id": "679Lmwt3l1Bk"
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jTX3d6qEYhvg"
   },
   "source": [
    "# Image Processing with Neural Network\n",
    "## Session 20 : torch FRCNN\n",
    "<img src='../../../prasami_images/prasami_color_tutorials_small.png' style = 'width:400px;' alt=\"By Pramod Sharma : pramod.sharma@prasami.com\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Faster R-CNN (FRCNN) is a deep learning model specifically designed for object detection tasks, meaning it identifies and classifies multiple objects within an image and draws bounding boxes around them. It builds on the original Region-Based Convolutional Neural Networks (R-CNN) and its predecessor Fast R-CNN, improving efficiency and accuracy. For this demo, we will be using PyTorch version of the Faster RCNN object detection model. They call it the Faster RCNN ResNet50 FPN V2.\n",
    "\n",
    "### Architecture of Faster R-CNN\n",
    "#### Convolutional Layers (Feature Extraction):\n",
    "\n",
    "- This first stage uses a convolutional neural network (usually a backbone like ResNet, VGG, or Inception) to extract feature maps from the input image.\n",
    "- These feature maps contain spatial information about the various objects and regions in the image.\n",
    "#### Region Proposal Network (RPN):\n",
    "\n",
    "- The RPN generates potential regions or object proposals. Itâ€™s a lightweight neural network that proposes regions likely to contain objects.\n",
    "- It outputs two results:\n",
    "    - Objectness scores: Probabilities that a region contains an object or background.\n",
    "    - Bounding box coordinates: Predictions for bounding box locations.\n",
    "- The RPN slides over the feature map, examining each location to predict regions likely to contain objects. Non-Maximum Suppression (NMS) is then applied to eliminate redundant proposals.\n",
    "#### ROI Pooling:\n",
    "\n",
    "- Faster R-CNN uses a technique called ROI (Region of Interest) Pooling, which extracts fixed-size feature maps from proposals.\n",
    "- ROI Pooling applies spatial transformation to maintain a consistent feature map size for each region, making them compatible with the next stage.\n",
    "#### Fully Connected Layers (Classification and Bounding Box Regression):\n",
    "\n",
    "- Each fixed-size feature map from ROI Pooling is passed to fully connected layers to classify the objects and refine the bounding box coordinates further.\n",
    "#### Output:\n",
    "\n",
    "The final output includes bounding box coordinates, classes, and a class label for each detected object in the image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = '../../../images/faster_R_CNN.png'>\n",
    "\n",
    "[Figure 2: Faster R-CNN is a single, unified network for object detection.](https://arxiv.org/pdf/1506.01497)\n",
    "\n",
    "The RPN module serves as the 'attention' of this unified network.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iAve6DCL4JH4"
   },
   "outputs": [],
   "source": [
    "### Import Libraries\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn_v2\n",
    "from torchvision.transforms import functional as F\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import cv2\n",
    "\n",
    "#from utils.helper import fn_plot_confusion_matrix, fn_plot_tf_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l-56YHubYhvq"
   },
   "outputs": [],
   "source": [
    "# Some basic parameters\n",
    "\n",
    "inpDir = '../../../input' # location where input data is stored\n",
    "outDir = '../output' # location to store outputs\n",
    "subDir = 'basic_operations' # location of the images\n",
    "modelDir = '../models'\n",
    "altName = 'rcnn'\n",
    "\n",
    "\n",
    "RANDOM_STATE = 24 # for initialization ----- REMEMBER: to remove at the time of promotion to production\n",
    "EPOCHS = 100 # number of cycles to run\n",
    "THRESHOLD = 0.8\n",
    "\n",
    "\n",
    "# Set parameters for decoration of plots\n",
    "params = {'legend.fontsize' : 'large',\n",
    "          'figure.figsize'  : (15,12),\n",
    "          'axes.labelsize'  : 'x-large',\n",
    "          'axes.titlesize'  :'x-large',\n",
    "          'xtick.labelsize' :'large',\n",
    "          'ytick.labelsize' :'large',\n",
    "         }\n",
    "\n",
    "CMAP = plt.cm.brg\n",
    "\n",
    "plt.rcParams.update(params) # update rcParams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All about CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Is CUDA available: ', torch.cuda.is_available())\n",
    "print ('CUDA version: ', torch.version.cuda )\n",
    "print ('Current Device ID: ', torch.cuda.current_device())\n",
    "print ('Name of the CUDA device: ', torch.cuda.get_device_name(torch.cuda.current_device()))\n",
    "\n",
    "# Get cpu or gpu device for training.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The COCO Dataset Category Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COCO_INSTANCE_CATEGORY_NAMES = [\n",
    "    '__background__', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus',\n",
    "    'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'N/A', 'stop sign',\n",
    "    'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow',\n",
    "    'elephant', 'bear', 'zebra', 'giraffe', 'N/A', 'backpack', 'umbrella', 'N/A', 'N/A',\n",
    "    'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball',\n",
    "    'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket',\n",
    "    'bottle', 'N/A', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl',\n",
    "    'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza',\n",
    "    'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'N/A', 'dining table',\n",
    "    'N/A', 'N/A', 'toilet', 'N/A', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone',\n",
    "    'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'N/A', 'book',\n",
    "    'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush'\n",
    "]\n",
    "\n",
    "# Create different colors for each class.\n",
    "COLORS = np.random.uniform(0, 255, size=(len(COCO_INSTANCE_CATEGORY_NAMES), 3))\n",
    "# Define the torchvision image transforms.\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Predict Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(image, model, device, detection_threshold = 0.5, coco_names=COCO_INSTANCE_CATEGORY_NAMES):\n",
    "    \"\"\"\n",
    "    Predict the output of an image after forward pass through\n",
    "    the model and return the bounding boxes, class names, and \n",
    "    class labels. \n",
    "    \"\"\"\n",
    "    # Transform the image to tensor.\n",
    "    image = transform(image).to(device)\n",
    "    # Add a batch dimension.\n",
    "    image = image.unsqueeze(0) \n",
    "    # Get the predictions on the image.\n",
    "    with torch.no_grad():\n",
    "        outputs = model(image) \n",
    "    # Get score for all the predicted objects.\n",
    "    pred_scores = outputs[0]['scores'].detach().cpu().numpy()\n",
    "    # Get all the predicted bounding boxes.\n",
    "    pred_bboxes = outputs[0]['boxes'].detach().cpu().numpy()\n",
    "    # Get boxes above the threshold score.\n",
    "    boxes = pred_bboxes[pred_scores >= detection_threshold].astype(np.int32)\n",
    "    labels = outputs[0]['labels'][:len(boxes)]\n",
    "    # Get all the predicted class names.\n",
    "    pred_classes = [coco_names[i] for i in labels.cpu().numpy()]\n",
    "    return boxes, pred_classes, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_boxes(boxes, classes, labels, image):\n",
    "    '''`\n",
    "    Draws bounding boxes with labels on an image to represent detected objects.\n",
    "\n",
    "    Args:\n",
    "        boxes (list): A list of bounding box coordinates for each detected object,\n",
    "                    where each box is represented by [x_min, y_min, x_max, y_max].\n",
    "        classes (list): A list of class names corresponding to each bounding box.\n",
    "        labels (list): A list of label indices for each detected object; these are \n",
    "                    used to pick specific colors for each class.\n",
    "        image (numpy array): The image on which to draw the bounding boxes.\n",
    "\n",
    "    Returns:\n",
    "        image (numpy array): The modified image with bounding boxes and labels drawn on it.\n",
    "    '''\n",
    "\n",
    "    # Determine line width for the bounding boxes based on image dimensions.\n",
    "    # This sets the line width to a fraction of the sum of image height and width,\n",
    "    # with a minimum width of 2 to ensure visibility.\n",
    "    lw = max(round(sum(image.shape) / 2 * 0.001), 2)\n",
    "\n",
    "    # Determine font thickness, set to one less than line width but with a minimum of 1.\n",
    "    tf = max(lw - 1, 2)\n",
    "\n",
    "    # Loop through each bounding box to draw it on the image.\n",
    "    for i, box in enumerate(boxes):\n",
    "        # Select a color for the bounding box from the COLORS array, using the label index.\n",
    "        # COLORS is a predefined array where each index corresponds to a specific color.\n",
    "        color = COLORS[labels[i]]\n",
    "\n",
    "        # Draw the bounding box as a rectangle on the image.\n",
    "        \n",
    "        cv2.rectangle(\n",
    "            img=image,\n",
    "            pt1=(int(box[0]), int(box[1])), # top-left corner of the box.\n",
    "            pt2=(int(box[2]), int(box[3])), # bottom-right corner of the box.\n",
    "            color=color[::-1],              # Reversed to BGR for OpenCV\n",
    "            thickness=lw                    # set to lw to maintain consistency with text.\n",
    "        )\n",
    "\n",
    "        # Add text to label the bounding box with the class name.        \n",
    "        cv2.putText(\n",
    "            img=image, \n",
    "            text=classes[i], \n",
    "            org=(int(box[0]), int(box[1] - 5)),  # 5 pixels above the top-left corner\n",
    "            fontFace=cv2.FONT_HERSHEY_SIMPLEX, \n",
    "            fontScale=lw / 2,                    # proportional to line width for a balanced appearance.\n",
    "            color=color[::-1],                   # Reversed to BGR for OpenCV\n",
    "            thickness=tf,                        # set to tf for good legibility.\n",
    "            lineType=cv2.LINE_AA                 # anti-aliased text for smoothness.\n",
    "        )\n",
    "\n",
    "    # Return the modified image with all bounding boxes and labels.\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a pre-trained Faster R-CNN model\n",
    "def get_model(device='cuda'):\n",
    "    \n",
    "    # Load the model.\n",
    "    model = fasterrcnn_resnet50_fpn_v2(weights='DEFAULT')\n",
    "\n",
    "    # Load the model onto the computation device.\n",
    "    model = model.eval().to(device)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pZ9KD8ZnYhv0"
   },
   "source": [
    "## Load Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1FxqsV2_Yhv1"
   },
   "outputs": [],
   "source": [
    "# Load the input image and apply transformations\n",
    "def load_image(image_path):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    \n",
    "    # Create a BGR copy of the image for annotation.\n",
    "    image_bgr = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    return image, image_bgr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgName = 'road_3.png' # 'IMG_1295.JPG'\n",
    "# Load an example image\n",
    "image_path = os.path.join(inpDir, subDir, imgName)  # specify your image path\n",
    "image, image_bgr = load_image(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(image_bgr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect outputs.\n",
    "with torch.no_grad():\n",
    "    boxes, classes, labels = predict(image, model, device, detection_threshold= THRESHOLD)\n",
    "# Draw bounding boxes.\n",
    "image = draw_boxes(boxes, classes, labels, image_bgr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(image_bgr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgbIm = cv2.cvtColor(image,cv2.COLOR_BGR2RGB)\n",
    "plt.imshow(rgbIm);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vidName = 'VID_20240320164819_F.MOV' # 'VID_20240929112441_F.MOV' # 'VID_20240320080723_F.MOV' #'VID_20240320164819_F.MOV' # 'VID_20240320164919_F.MOV'\n",
    "vidFilePath = os.path.join(inpDir, subDir, vidName)\n",
    "\n",
    "cap = cv2.VideoCapture(vidFilePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (cap.isOpened() == False):\n",
    "    print('Error while trying to read video. Please check path again')\n",
    "# Get the frame width and height.\n",
    "frame_width = int(cap.get(3))\n",
    "frame_height = int(cap.get(4))\n",
    "\n",
    "# for saving the file\n",
    "save_name = f\"{vidFilePath[-1].split('.')[0]}_t{''.join(str(THRESHOLD).split('.'))}_{altName}\"\n",
    "# Define codec and create VideoWriter object .\n",
    "out = cv2.VideoWriter(os.path.join(outDir, f\"{save_name}.mp4\"), \n",
    "                      cv2.VideoWriter_fourcc(*'mp4v'), 30, \n",
    "                      (frame_width, frame_height))\n",
    "frame_count = 0 # To count total frames.\n",
    "total_fps = 0 # To get the final frames per second."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read until end of video.\n",
    "while(cap.isOpened):\n",
    "    # Capture each frame of the video.\n",
    "    ret, frame = cap.read()\n",
    "    if ret:\n",
    "        frame_copy = frame.copy()\n",
    "        frame_copy = cv2.cvtColor(frame_copy, cv2.COLOR_BGR2RGB)\n",
    "        # Get the start time.\n",
    "        start_time = time.time()\n",
    "        with torch.no_grad():\n",
    "            # Get predictions for the current frame.\n",
    "            boxes, classes, labels = predict( frame, model, device, THRESHOLD)\n",
    "        \n",
    "        # Draw boxes and show current frame on screen.\n",
    "        image = draw_boxes(boxes, classes, labels, frame)\n",
    "        # Get the end time.\n",
    "        end_time = time.time()\n",
    "        # Get the fps.\n",
    "        fps = 1 / (end_time - start_time)\n",
    "        # Add fps to total fps.\n",
    "        total_fps += fps\n",
    "        # Increment frame count.\n",
    "        frame_count += 1\n",
    "        # Write the FPS on the current frame.\n",
    "        cv2.putText(\n",
    "            img=image, \n",
    "            text=f\"{fps:.3f} FPS\", \n",
    "            org=(15, 30), \n",
    "            fontFace=cv2.FONT_HERSHEY_SIMPLEX,\n",
    "            fontScale=1, \n",
    "            color=(0, 255, 0), \n",
    "            thickness=2,\n",
    "            lineType=cv2.LINE_AA\n",
    "        )\n",
    "        # Convert from BGR to RGB color format.\n",
    "        cv2.imshow('image', image)\n",
    "        out.write(image)\n",
    "        # Press `q` to exit.\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    else:\n",
    "        break\n",
    "# Release VideoCapture().\n",
    "cap.release()\n",
    "# Close all frames and video windows.\n",
    "cv2.destroyAllWindows()\n",
    "# Calculate and print the average FPS.\n",
    "avg_fps = total_fps / frame_count\n",
    "print(f\"Average FPS: {avg_fps:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "S25a_dnn_cnn_l2_flowers_wip.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
