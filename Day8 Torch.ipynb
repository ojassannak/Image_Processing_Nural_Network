{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44adfd52-a482-4a7d-9aff-50ce0bddb0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "###-----------------\n",
    "### Import Libraries\n",
    "###-----------------\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import (accuracy_score, classification_report)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader , Dataset\n",
    "\n",
    "#from utils.helper import fn_plot_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6f8376d-7f8f-4f24-894c-f87790331920",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_plot_tf_hist(hist_df : pd.DataFrame):\n",
    "    '''\n",
    "    Note this function is specifically designed to plot Tensorflow training output\n",
    "    Args:\n",
    "      hist_df : pandas DataFrame with four columns\n",
    "                For 'x' values, we will use index\n",
    "                first column is accuracy\n",
    "                Second column is loss\n",
    "                third column is val_accuracy\n",
    "                fourth column is val_loss\n",
    "    '''\n",
    "    fig, axes = plt.subplots(1,2 , figsize = (15,6)) # instantiate plot\n",
    "\n",
    "    # properties  matplotlib.patch.Patch \n",
    "    props = dict(boxstyle='round', facecolor='aqua', alpha=0.4)\n",
    "    facecolor = 'cyan'\n",
    "    fontsize=12\n",
    "    \n",
    "    # Get columns by index to eliminate any column naming error\n",
    "    y1 = hist_df.columns[0]\n",
    "    y2 = hist_df.columns[1]\n",
    "    y3 = hist_df.columns[2]\n",
    "    y4 = hist_df.columns[3]\n",
    "\n",
    "    # Where was min loss\n",
    "    best = hist_df[hist_df[y4] == hist_df[y4].min()]\n",
    " \n",
    "    ax = axes[0]\n",
    "\n",
    "    hist_df.plot(y = [y2,y4], ax = ax, colormap=CMAP)\n",
    "\n",
    "\n",
    "    # little beautification\n",
    "    txtFmt = \"Loss: \\n  train: {:6.4f}\\n   test: {:6.4f}\"\n",
    "    txtstr = txtFmt.format(hist_df.iloc[-1][y2],\n",
    "                           hist_df.iloc[-1][y4]) #text to plot\n",
    "    \n",
    "    # place a text box in upper middle in axes coords\n",
    "    ax.text(0.3, 0.95, txtstr, transform=ax.transAxes, fontsize=fontsize,\n",
    "            verticalalignment='top', bbox=props)\n",
    "\n",
    "    # Mark arrow at lowest\n",
    "    ax.annotate(f'Min: {best[y4].to_numpy()[0]:6.4f}', # text to print\n",
    "                xy=(best.index.to_numpy(), best[y4].to_numpy()[0]), # Arrow start\n",
    "                xytext=(best.index.to_numpy()-1, best[y4].to_numpy()[0]), # location of text \n",
    "                fontsize=fontsize, va='bottom', ha='right',bbox=props, # beautification of text\n",
    "                arrowprops=dict(facecolor=facecolor, shrink=0.05)) # arrow\n",
    "\n",
    "    # Draw vertical line at best value\n",
    "    ax.axvline(x = best.index.to_numpy(), color = 'green', linestyle='-.', lw = 3);\n",
    "\n",
    "    ax.set_xlabel(\"Epochs\")\n",
    "    ax.set_ylabel(y2.capitalize())\n",
    "    ax.set_title('Errors')\n",
    "    ax.legend(loc = 'upper left') # model legend to upper left\n",
    "\n",
    "    ax = axes[1]\n",
    "\n",
    "    hist_df.plot( y = [y1, y3], ax = ax, colormap=CMAP)\n",
    "    \n",
    "    # little beautification\n",
    "    txtFmt = \"Accuracy: \\n  train: {:6.4f}\\n  test:  {:6.4f}\"\n",
    "    txtstr = txtFmt.format(hist_df.iloc[-1][y1],\n",
    "                           hist_df.iloc[-1][y3]) #text to plot\n",
    "\n",
    "    # place a text box in upper middle in axes coords\n",
    "    ax.text(0.3, 0.2, txtstr, transform=ax.transAxes, fontsize=fontsize,\n",
    "            verticalalignment='top', bbox=props)\n",
    "\n",
    "    # Mark arrow at lowest\n",
    "    ax.annotate(f'Best: {best[y3].to_numpy()[0]:6.4f}', # text to print\n",
    "                xy=(best.index.to_numpy(), best[y3].to_numpy()[0]), # Arrow start\n",
    "                xytext=(best.index.to_numpy()-1, best[y3].to_numpy()[0]), # location of text \n",
    "                fontsize=fontsize, va='bottom', ha='right',bbox=props, # beautification of text\n",
    "                arrowprops=dict(facecolor=facecolor, shrink=0.05)) # arrow\n",
    "    \n",
    "    \n",
    "    # Draw vertical line at best value\n",
    "    ax.axvline(x = best.index.to_numpy(), color = 'green', linestyle='-.', lw = 3);\n",
    "\n",
    "    ax.set_xlabel(\"Epochs\")\n",
    "    ax.set_ylabel(y1.capitalize())\n",
    "    ax.legend(loc = 'lower left')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "\n",
    "\n",
    "def fn_plot_confusion_matrix(y_true, y_pred, labels):\n",
    "    '''\n",
    "    Args:\n",
    "        y_true: Ground Truth \n",
    "        y_pred : Predictions\n",
    "        labels : dictionary \n",
    "                  {0: 'Goal Keeper', \n",
    "                  1: 'Defender', \n",
    "                  2: 'Mid-Fielder', \n",
    "                  3: 'Forward'}\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    cm  = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
    "                                  display_labels=labels.values())\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize = (4,4))\n",
    "    \n",
    "    disp.plot(ax = ax, cmap = 'Blues', xticks_rotation = 'vertical', colorbar=False)\n",
    "    # Disable the grid\n",
    "    ax.grid(False)\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63390cd9-8d72-47b0-b8a0-08be2f90f983",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some basic parameters\n",
    "inpDir = '../../../input' # location where input data is stored\n",
    "outDir = '../output' # location to store outputs\n",
    "subDir = 'fashion_MNIST' # location of the images\n",
    "modelDir = '../models'\n",
    "altName = 'all_in'\n",
    "\n",
    "RANDOM_STATE = 24 # for initialization ----- REMEMBER: to remove at the time of promotion to production\n",
    "BATCH_SIZE = 32 # batch size for training   \n",
    "TRAIN_SIZE = BATCH_SIZE * 9\n",
    "\n",
    "EPOCHS = 100 # number of cycles to run\n",
    "ALPHA = 0.001 # learning rate\n",
    "WEIGHT_DECAY =0.001\n",
    "LR_FACTOR = 0.1\n",
    "LR_PATIENCE = 10\n",
    "PATIENCE = 20\n",
    "\n",
    "# Set parameters for decoration of plots\n",
    "params = {'legend.fontsize' : 'large',\n",
    "          'figure.figsize'  : (9,9),\n",
    "          'axes.labelsize'  : 'x-large',\n",
    "          'axes.titlesize'  :'x-large',\n",
    "          'xtick.labelsize' :'large',\n",
    "          'ytick.labelsize' :'large',\n",
    "         }\n",
    "\n",
    "plt.rcParams.update(params) # update rcParams\n",
    "CMAP = plt.cm.coolwarm\n",
    "plt.style.use('seaborn-v0_8-darkgrid') # plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd6d2e5a-51cb-4562-bb6d-6a90f46af2a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.99539</td>\n",
       "      <td>-0.05889</td>\n",
       "      <td>0.85243</td>\n",
       "      <td>0.02306</td>\n",
       "      <td>0.83398</td>\n",
       "      <td>-0.37708</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.03760</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.51171</td>\n",
       "      <td>0.41078</td>\n",
       "      <td>-0.46168</td>\n",
       "      <td>0.21266</td>\n",
       "      <td>-0.34090</td>\n",
       "      <td>0.42267</td>\n",
       "      <td>-0.54487</td>\n",
       "      <td>0.18641</td>\n",
       "      <td>-0.45300</td>\n",
       "      <td>g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>-0.18829</td>\n",
       "      <td>0.93035</td>\n",
       "      <td>-0.36156</td>\n",
       "      <td>-0.10868</td>\n",
       "      <td>-0.93597</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>-0.04549</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.26569</td>\n",
       "      <td>-0.20468</td>\n",
       "      <td>-0.18401</td>\n",
       "      <td>-0.19040</td>\n",
       "      <td>-0.11593</td>\n",
       "      <td>-0.16626</td>\n",
       "      <td>-0.06288</td>\n",
       "      <td>-0.13738</td>\n",
       "      <td>-0.02447</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>-0.03365</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.00485</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>-0.12062</td>\n",
       "      <td>0.88965</td>\n",
       "      <td>0.01198</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.40220</td>\n",
       "      <td>0.58984</td>\n",
       "      <td>-0.22145</td>\n",
       "      <td>0.43100</td>\n",
       "      <td>-0.17365</td>\n",
       "      <td>0.60436</td>\n",
       "      <td>-0.24180</td>\n",
       "      <td>0.56045</td>\n",
       "      <td>-0.38238</td>\n",
       "      <td>g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>-0.45161</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.71216</td>\n",
       "      <td>-1.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.90695</td>\n",
       "      <td>0.51613</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>-0.20099</td>\n",
       "      <td>0.25682</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>-0.32382</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>-0.02401</td>\n",
       "      <td>0.94140</td>\n",
       "      <td>0.06531</td>\n",
       "      <td>0.92106</td>\n",
       "      <td>-0.23255</td>\n",
       "      <td>0.77152</td>\n",
       "      <td>-0.16399</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.65158</td>\n",
       "      <td>0.13290</td>\n",
       "      <td>-0.53206</td>\n",
       "      <td>0.02431</td>\n",
       "      <td>-0.62197</td>\n",
       "      <td>-0.05707</td>\n",
       "      <td>-0.59573</td>\n",
       "      <td>-0.04608</td>\n",
       "      <td>-0.65697</td>\n",
       "      <td>g</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0   1        2        3        4        5        6        7        8   \\\n",
       "0   1   0  0.99539 -0.05889  0.85243  0.02306  0.83398 -0.37708  1.00000   \n",
       "1   1   0  1.00000 -0.18829  0.93035 -0.36156 -0.10868 -0.93597  1.00000   \n",
       "2   1   0  1.00000 -0.03365  1.00000  0.00485  1.00000 -0.12062  0.88965   \n",
       "3   1   0  1.00000 -0.45161  1.00000  1.00000  0.71216 -1.00000  0.00000   \n",
       "4   1   0  1.00000 -0.02401  0.94140  0.06531  0.92106 -0.23255  0.77152   \n",
       "\n",
       "        9   ...       25       26       27       28       29       30  \\\n",
       "0  0.03760  ... -0.51171  0.41078 -0.46168  0.21266 -0.34090  0.42267   \n",
       "1 -0.04549  ... -0.26569 -0.20468 -0.18401 -0.19040 -0.11593 -0.16626   \n",
       "2  0.01198  ... -0.40220  0.58984 -0.22145  0.43100 -0.17365  0.60436   \n",
       "3  0.00000  ...  0.90695  0.51613  1.00000  1.00000 -0.20099  0.25682   \n",
       "4 -0.16399  ... -0.65158  0.13290 -0.53206  0.02431 -0.62197 -0.05707   \n",
       "\n",
       "        31       32       33  34  \n",
       "0 -0.54487  0.18641 -0.45300   g  \n",
       "1 -0.06288 -0.13738 -0.02447   b  \n",
       "2 -0.24180  0.56045 -0.38238   g  \n",
       "3  1.00000 -0.32382  1.00000   b  \n",
       "4 -0.59573 -0.04608 -0.65697   g  \n",
       "\n",
       "[5 rows x 35 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df = pd.read_csv('ionosphere.data', header=None)\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da01e4f1-7e2c-48d9-ba2b-a3dd4f2f929e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((288, 35), (63, 35))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df, test_df = train_test_split(data_df,\n",
    "                                     train_size=TRAIN_SIZE,\n",
    "                                     stratify= data_df[data_df.columns[-1]],\n",
    "                                     random_state=RANDOM_STATE)\n",
    "\n",
    "train_df.shape , test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e57f96a-dc48-4d71-9a34-12c31adce194",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "device = 'code'  if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Using {device} device')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dffe41f7-a92c-4181-8c63-2f790a760bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Using Sinleton class Pattenn \"\"\"\n",
    "\n",
    "class Transformers:\n",
    "    _instance = None\n",
    "\n",
    "    def __init__ (self):\n",
    "        if Transformers._instance is not None:\n",
    "            raise Exception (\"Global Scaler claass is singleton\")\n",
    "\n",
    "        self.scaler = StandardScaler()\n",
    "        self.encoder = LabelEncoder()\n",
    "\n",
    "    @classmethod\n",
    "    def get_instance(cls):\n",
    "        if cls._instance is None:\n",
    "            cls._instance = Transformers()\n",
    "        return cls. _instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc0bd3af-729f-447b-a6f1-758cdcd6bb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class  IonoDS(Dataset):\n",
    "    transformers = Transformers.get_instance()\n",
    "\n",
    "    def __init__(self,\n",
    "                dataframe: pd.DataFrame,\n",
    "                device: str = device,\n",
    "                is_train= True,\n",
    "                label_col =None):\n",
    "        super(IonoDS, self).__init__()\n",
    "\n",
    "        self.df = dataframe\n",
    "        self.device = device\n",
    "        self.is_train = is_train\n",
    "        self.scaler = self.transformers.scaler\n",
    "        self.encoder = self.transformers.encoder\n",
    "        self.label_col = label_col\n",
    "\n",
    "        y = self.df[label_col].to_numpy()\n",
    "        x = self.df.drop(label_col, axis = 1)\n",
    "\n",
    "        if self.is_train:\n",
    "            self.labels = self.encoder.fit_transform(y)\n",
    "            self.features = self.scaler.fit_transform(x)\n",
    "        else:\n",
    "            self.labels = self.encoder.transform(y)\n",
    "            self.features = self.scaler.transform(x)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        #Extract features and labels from the dataframe row\n",
    "        features = self.features[index]\n",
    "        label = self.labels[index]\n",
    "\n",
    "        features = torch.tensor(features, dtype = torch.float32, device = self.device)\n",
    "        label = torch.tensor(label, dtype= torch.int64, device = self.device)\n",
    "\n",
    "        return features, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5c340696-c0c5-46e3-a31f-36fef4710e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "        dor1 = 0.05\n",
    "        dor2 = 0.15\n",
    "        dor3 = 0.25\n",
    "\n",
    "        self.layer1 = nn.Linear(input_dim,26)\n",
    "        self.bm1 = nn.BatchNorm1d(26)\n",
    "        self.do1 = nn.Dropout(dor1)\n",
    "        self.act1 = nn.ReLU()\n",
    "\n",
    "        self.layer2 = nn.Linear(26,18)\n",
    "        self.bm2 = nn.BatchNorm1d(18)\n",
    "        self.do2 = nn.Dropout(dor2)\n",
    "        self.act2 = nn.ReLU()\n",
    "\n",
    "        self.layer3 = nn.Linear(18, 10)\n",
    "        self.bm3 = nn.BatchNorm1d(10)\n",
    "        self.do3 = nn.Dropout(dor3)\n",
    "        self.act3 = nn.ReLU()\n",
    "\n",
    "        self.layer4 = nn.Linear(10,2)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.do1(self.act1 (self.bm1 (self.layer1(x))))\n",
    "        x = self.do2(self.act2 (self.bm2 (self.layer2(x))))\n",
    "        x = self.do3(self.act3 (self.bm3 (self.layer3(x))))\n",
    "\n",
    "        output = self.softmax(self.layer4(x))\n",
    "\n",
    "        return output\n",
    "\n",
    "input_dim = 34\n",
    "model = Model(input_dim).to(device)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "50009df0-a9d3-489c-8c35-3e69923df323",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_col = 34\n",
    "train_ds = IonoDS(train_df, is_train = True,  label_col = label_col)\n",
    "test_ds = IonoDS(test_df, is_train = False,  label_col = label_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6cae585b-3ce1-4680-bd41-b5272cd376e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d2c15feb-5b48-4553-a837-658f424663ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch   0 | Loss: 0.837359/0.765338 | Acc: 0.347222/0.365079\n",
      "At epoch  10 | Loss: 0.330214/0.425065 | Acc: 0.885417/0.809524\n",
      "At epoch  20 | Loss: 0.090054/0.258848 | Acc: 0.968750/0.873016\n",
      "At epoch  30 | Loss: 0.037765/0.216319 | Acc: 0.989583/0.888889\n",
      "At epoch  40 | Loss: 0.019894/0.191054 | Acc: 0.996528/0.920635\n",
      "At epoch  50 | Loss: 0.010879/0.186821 | Acc: 0.996528/0.936508\n",
      "At epoch  60 | Loss: 0.005417/0.194012 | Acc: 1.000000/0.936508\n",
      "At epoch  70 | Loss: 0.003596/0.204367 | Acc: 1.000000/0.936508\n"
     ]
    }
   ],
   "source": [
    "# Define the loss function and optimizer\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Initialize lists to track losses, accuracies, and epochs\n",
    "loss, tloss = [], []\n",
    "acc, tacc = [], []\n",
    "n_epoch = []\n",
    "\n",
    "# Define the optimizer and learning rate scheduler\n",
    "optimizer = torch.optim.Adam(model.parameters(),\n",
    "                             lr=ALPHA,\n",
    "                             weight_decay=0.1e-5)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,\n",
    "                                                       mode='min',\n",
    "                                                       factor=LR_FACTOR,\n",
    "                                                       patience=LR_PATIENCE,\n",
    "                                                       min_lr=1e-5)\n",
    "\n",
    "minLoss = float('inf')\n",
    "\n",
    "# Define the path to save the model\n",
    "savePath = os.path.join(modelDir, subDir, \"iono.path\")\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss = 0.0\n",
    "    train_acc = 0.0\n",
    "\n",
    "    # Training phase\n",
    "    for i, data in enumerate(train_loader):\n",
    "        inputs, labels = data\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        batch_loss = loss_fn(outputs, labels)\n",
    "        batch_acc = accuracy_score(labels.cpu().numpy(), preds.cpu().numpy())\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update training loss and accuracy\n",
    "        train_loss += batch_loss.item() * inputs.size(0)\n",
    "        train_acc += batch_acc * inputs.size(0)\n",
    "\n",
    "    # Average training loss and accuracy\n",
    "    train_loss /= len(train_ds)\n",
    "    train_acc /= len(train_ds)\n",
    "\n",
    "    # Store training loss and accuracy\n",
    "    loss.append(train_loss)\n",
    "    acc.append(train_acc)\n",
    "\n",
    "    # Test phase\n",
    "    test_loss = 0.0\n",
    "    test_acc = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for data in test_loader:\n",
    "            inputs, labels = data\n",
    "            outputs = model(inputs)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            batch_loss = loss_fn(outputs, labels)\n",
    "            batch_acc = accuracy_score(labels.cpu().numpy(), preds.cpu().numpy())\n",
    "            test_loss += batch_loss.item() * inputs.size(0)\n",
    "            test_acc += batch_acc * inputs.size(0)\n",
    "\n",
    "        # Average test loss and accuracy\n",
    "        test_loss /= len(test_ds)\n",
    "        test_acc /= len(test_ds)\n",
    "\n",
    "        # Store test loss and accuracy\n",
    "        tloss.append(test_loss)\n",
    "        tacc.append(test_acc)\n",
    "\n",
    "    # Store the current epoch\n",
    "    n_epoch.append(epoch)\n",
    "\n",
    "    # Step the learning rate scheduler based on test loss\n",
    "    scheduler.step(test_loss)\n",
    "\n",
    "    # If the current test loss is lower, save the model\n",
    "    if test_loss < minLoss:\n",
    "        minLoss = test_loss\n",
    "        counter = 0\n",
    "        # Save the model checkpoint\n",
    "        torch.save({\n",
    "            \"epoch\": epoch + 1,\n",
    "            \"model_state_dict\": model.state_dict(),\n",
    "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "            \"loss\": loss_fn\n",
    "        }, savePath)\n",
    "    else:\n",
    "        counter += 1\n",
    "        # Stop training if the loss hasn't improved for `PATIENCE` epochs\n",
    "        if counter > PATIENCE:\n",
    "            break\n",
    "\n",
    "    # Print status every 10 epochs\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'At epoch {epoch:3d} | Loss: {train_loss:4f}/{test_loss:4f} | Acc: {train_acc:4f}/{test_acc:4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e808284e-32f6-4580-bd36-c432cdbe6e65",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
